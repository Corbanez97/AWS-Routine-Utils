{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac8fc266-cde9-4bc8-b64a-16d155e16117",
   "metadata": {
    "tags": []
   },
   "source": [
    "# S3 Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a914431-fbfe-4da7-bade-07e410dba303",
   "metadata": {},
   "source": [
    "This class is responsible for connecting to an **S3 Bucket**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463e1df9-8e62-4fc2-bccf-5ef684cd74ed",
   "metadata": {},
   "source": [
    "## S3 FileSystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae92e863-fd46-4701-802b-346d6eeefdf8",
   "metadata": {},
   "source": [
    "**S3FileSystem** builds on **aiobotocore** to provide a convenient Python filesystem interface for **S3**. In other words, this package simulates a folder structure on our work directory that handles needed **S3 API** calls just like our local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7598f2de-13f0-4aa2-924c-9b3e2cd5891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import json\n",
    "\n",
    "with open(r'../connections/connex.json', 'r') as f:\n",
    "    connex = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b0b61-3895-4e22-b0ba-925a882ac6f6",
   "metadata": {},
   "source": [
    "By creating a **S3FileSystem** object, we are capable of oppening files on **S3** just like we do locally. Not only open them, but write and read information on these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4488860-12fa-4941-8695-691b9f0ecb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(key = connex['DEV']['s3']['key'], secret = connex['DEV']['s3']['secret']) \n",
    "\n",
    "fs.ls('lake-s3-dev')\n",
    "\n",
    "with fs._open(path = 'lake-s3-dev/landing/first_s3fs_load/test.csv', mode = 'wb') as f:\n",
    "    f.write(b'test,test2,test3\\n1,2,3\\na,b,c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d91013d-1c76-4505-b4cd-59a6de800576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3Connection:\n",
    "    \n",
    "    def __init__(self, bucket_name: str, key: str, secret: str) -> None:\n",
    "        self.bucket_name = bucket_name\n",
    "        self.s3 = s3fs.S3FileSystem(key = key, secret = secret)\n",
    "        \n",
    "    def get(self, source_path: str):\n",
    "        return self.s3._open(path = source_path, mode = 'rb')\n",
    "        \n",
    "    def upload(self , source_file, final_path: str):\n",
    "        self.file = source_file\n",
    "        self.final_path = self.s3._open(path = final_path, mode = 'wb')\n",
    "        self.final_path.write(source_file)\n",
    "        \n",
    "    def move(self, source_path: str, final_path: str):\n",
    "        self.source_path = self.s3._open(path = source_path, mode = 'rb')\n",
    "        self.final_path = self.s3._open(path = final_path, mode = 'wb')\n",
    "        \n",
    "        self.final_path.write(self.source_source)\n",
    "    \n",
    "    def delete(self, source_path: str):\n",
    "        return self.s3._rm_file(path = source_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41310193-7650-43b2-a916-241c34aab538",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Executors and Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f597d38b-e0ff-406b-8eaa-08344b296d26",
   "metadata": {},
   "source": [
    "This notebook has a brief explanation of how we are going to create generic **Classes** capable of handling important Data Lake jobs. *E.g.*, transferring data from different storages, creating tables on **AWS Athena** and **AWS Redshift**, managing **Glue Jobs**, *etc*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826317a0-cf3b-4784-972e-7b52bc84cd4a",
   "metadata": {},
   "source": [
    "## Staging Data with a optional transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd4d55-1968-4e25-917b-f3228e494a75",
   "metadata": {},
   "source": [
    "The class we are creating below is capable of taking data from a parent directory and moving it to a dump directory. Furthermore, given a python script, it will run it with its respective arguments. This is a good option for a before hand cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "262fd28d-b200-4224-8216-f5526a513fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Local StagingExecutor for testing\n",
    "\n",
    "class StagingExecutor_alpha:\n",
    "    \n",
    "    def __init__(self, parent_directory: str, dump_directory: str, archive_or_delete: str = \"archive\", py_exec_path: str = None, py_exec_args: dict = None) -> None:\n",
    "        self.parent = parent_directory\n",
    "        self.dump = dump_directory\n",
    "        self.archive_or_delete = archive_or_delete\n",
    "        self.py_exec_path = py_exec_path\n",
    "        self.py_exec_args = py_exec_args\n",
    "\n",
    "    def transfer(self) -> None:\n",
    "        \n",
    "        if not self.py_exec_path:\n",
    "            print('No transformation required. Moving file using only parent and dump')\n",
    "        \n",
    "        elif self.py_exec_path:\n",
    "            import sys\n",
    "            sys.path.insert(1, self.py_exec_path)\n",
    "            import py_exec\n",
    "            \n",
    "            if not self.py_exec_args:\n",
    "                print('Python Executor does not require arguments')\n",
    "            \n",
    "            elif self.py_exec_args:\n",
    "                print(f'Python Executor is running with the following parameters:\\n{self.py_exec_args}')\n",
    "                py_exec.main(self.parent, self.dump, **self.py_exec_args)\n",
    "                \n",
    "    def post_staging(self) -> None:\n",
    "                \n",
    "        if self.archive_or_delete == 'archive':\n",
    "            print('File from landing will be moved to archive folder')\n",
    "        \n",
    "        elif self.archive_or_delete == 'delete':\n",
    "            print('File will be deleted from landing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "044d7a80-b279-4259-a90f-dce8174db625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StagingExecutor:\n",
    "    \n",
    "    def __init__(self, parent_directory: str, dump_directory: str, archive_or_delete: str = \"archive\", py_exec_path: str = None, py_exec_args: dict = None) -> None:\n",
    "        self.parent = parent_directory\n",
    "        self.dump = dump_directory\n",
    "        self.archive_or_delete = archive_or_delete\n",
    "        self.py_exec_path = py_exec_path\n",
    "        self.py_exec_args = py_exec_args\n",
    "\n",
    "    def transfer(self) -> None:\n",
    "        \n",
    "        if not self.py_exec_path:\n",
    "            print('No transformation required. Moving file using only parent and dump')\n",
    "        \n",
    "        elif self.py_exec_path:\n",
    "            import sys\n",
    "            sys.path.insert(1, self.py_exec_path)\n",
    "            import py_exec\n",
    "            \n",
    "            if not self.py_exec_args:\n",
    "                print('Python Executor does not require arguments')\n",
    "            \n",
    "            elif self.py_exec_args:\n",
    "                print(f'Python Executor is running with the following parameters:\\n{self.py_exec_args}')\n",
    "                py_exec.main(self.parent, self.dump, **self.py_exec_args)\n",
    "                \n",
    "    def post_staging(self) -> None:\n",
    "                \n",
    "        if self.archive_or_delete == 'archive':\n",
    "            print('File from landing will be moved to archive folder')\n",
    "        \n",
    "        elif self.archive_or_delete == 'delete':\n",
    "            print('File will be deleted from landing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9412a50-840b-457c-86bb-e98f35a8729f",
   "metadata": {},
   "source": [
    "## Downloader with unzip functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdeaa32-3c63-4c7d-86cf-fd3d7e06087e",
   "metadata": {},
   "source": [
    "This executor will take directories as main arguments, and download them into a landing folder (local or on **S3**) using *requests*. Notwithstanding downloading, it can be set to unzip and organize raw data for a cleaner staging task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dfa0d52-ec85-41c9-b5be-dbeef60f1d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "\n",
    "\n",
    "class DownloaderExecutor:\n",
    "    def __init__(self, requests_arguments: dict, landing_directory: str, unzip: str = None) -> None:\n",
    "        self.requests_arguments = requests_arguments ##A dictionary such as: {\"url\": URL, \"params\": PARAMS, ...}\n",
    "        self.landing_directory = landing_directory\n",
    "        self.unzip = unzip\n",
    "        self.res = None\n",
    "        ## this atribute should be set up by the DataLake/DataWarehouse class\n",
    "        self.s3 = S3Connection(bucket_name = 'lake-s3-dev', key = connex['DEV']['s3']['key'], secret = connex['DEV']['s3']['secret'])\n",
    "        \n",
    "        \n",
    "    def download(self):\n",
    "        self.res = requests.get(**self.requests_arguments)\n",
    "        \n",
    "        if self.unzip == \"unzip\":\n",
    "            print('Will have to unzip')\n",
    "            temp = zipfile.ZipFile(io.BytesIO(self.res.content))\n",
    "            temp.extractall('../data/temp/' + self.landing_directory.split(\"/\")[-2] + '/')\n",
    "            extracted_file = os.listdir('../data/temp/' + self.landing_directory.split(\"/\")[-2] + '/')[0]\n",
    "            with open('../data/temp/' + self.landing_directory.split(\"/\")[-2] + '/' + extracted_file, 'rb') as extracted_data:\n",
    "                self.s3.upload(extracted_data.read(), self.landing_directory)\n",
    "            shutil.rmtree('../data/temp/' + self.landing_directory.split(\"/\")[-2])\n",
    "            \n",
    "        elif not self.unzip:\n",
    "            print('Does not unzip')\n",
    "            self.s3 = S3Connection(bucket_name = 'lake-s3-dev', key = connex['DEV']['s3']['key'], secret = connex['DEV']['s3']['secret'])\n",
    "            self.s3.upload(self.res.content, self.landing_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497122b4-3946-4267-828b-397d6d1f2872",
   "metadata": {},
   "source": [
    "To test de unzip functionallity, we are using the following dataset: https://www.stats.govt.nz/assets/Uploads/Retail-trade-survey/Retail-trade-survey-September-2020-quarter/Download-data/retail-trade-survey-september-2020-quarter-csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d387011-5eab-4388-ab58-60b3a5523139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will have to unzip\n"
     ]
    }
   ],
   "source": [
    "downexec = {\"requests_arguments\": {\"url\": \"https://www.stats.govt.nz/assets/Uploads/Retail-trade-survey/Retail-trade-survey-September-2020-quarter/Download-data/retail-trade-survey-september-2020-quarter-csv.zip\"}, \"landing_directory\": \"lake-s3-dev/landing/retail_trade_survey/retail_trade_survey.csv\", \"unzip\": \"unzip\"}\n",
    "klass = DownloaderExecutor(**downexec)\n",
    "klass.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175257bd-2117-48f5-8d82-d595a2a15b07",
   "metadata": {},
   "source": [
    "# Orchestrator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2446b1f-f7fa-433a-86e0-a248a2fe340c",
   "metadata": {},
   "source": [
    "Data orchestration is a relatively new concept to describe the set of technologies that abstracts data access across storage systems, virtualizes all the data, and presents the data via standardized APIs with a global namespace to data-driven applications. There is a clear need for data orchestration because of the increasing complexity of the data ecosystem due to new frameworks, cloud adoption/migration, as well as the rise of data-driven applications. [[Data Orchestrator]](https://dzone.com/articles/data-orchestration-its-open-source-but-what-is-it)\n",
    "\n",
    "The Orchestrator is a class that will follow the routine_config.json, where one will declare which executors and their respective tasks to run. The model for our routine_config is:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"routine_name\": <ROUTINE_NAME>,\n",
    "    \"executors\": {\n",
    "        <EXECUTOR_CLASS>:  {\n",
    "            \"params\":<__init__ PARAMETERS>,\n",
    "            \"tasks\": <LIST_SELECTED_TASKS_FROM_EXECUTOR>\n",
    "        }\n",
    "    }\n",
    "}   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "623ac9f6-4624-4030-aad7-c0d060e77cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Orchestrator:\n",
    "    \n",
    "    def __init__(self, routine_config: dict) -> None:\n",
    "        self.routine_config = routine_config\n",
    "    \n",
    "    def run_executors(self):\n",
    "        for executor in routine_config['executors']:\n",
    "            self.executor_name = executor\n",
    "            klass = globals()[self.executor_name]\n",
    "            self.executor = klass(**routine_config['executors'][self.executor_name]['params'])\n",
    "            self.run_tasks()\n",
    "            \n",
    "    def run_tasks(self):\n",
    "        for task in routine_config['executors'][self.executor_name]['tasks']:\n",
    "            getattr(self.executor, task)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb3c54f-fb33-46cf-9bd0-8996685ac9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
