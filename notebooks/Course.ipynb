{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2b21ae2-51bf-475c-910f-39df8318dfaa",
   "metadata": {},
   "source": [
    "# [Data Engineering using Amazon Web Services](https://www.udemy.com/course/data-engineering-using-aws-analytics-services/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d304d24-98ff-49fc-b92c-eb094a1fb9c6",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58f802e-6b84-4194-b973-edab82749a2b",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## [Docker](https://www.docker.com/resources/what-container/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b71cc90-93a7-43a0-944b-bfbe5204464c",
   "metadata": {},
   "source": [
    "A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/docker.png\" /></div>\n",
    "\n",
    "Container images become containers at runtime and in the case of Docker containers – images become containers when they run on Docker Engine. Available for both Linux and Windows-based applications, containerized software will always run the same, regardless of the infrastructure. Containers isolate software from its environment and ensure that it works uniformly despite differences for instance between development and staging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31269106-879a-4c5a-ae3a-f5973a69b0b0",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efaff38-9dc0-4ea8-9781-03e088a696bf",
   "metadata": {},
   "source": [
    "First, make sure **Docker** is installed in your system using ***sudo apt-get install docker***, and you are logged in to your account with ***docker login***. To make sure everything is working fine, execute the ***docker run hello-world*** command.\n",
    "\n",
    "As an example, we've created a simple [Dockerfile](https://u.group/thinking/how-to-put-jupyter-notebooks-in-a-dockerfile/) on which we declare a sequence of operations to be followed. These operations will be executed once one builds the container in their machine. In this example, we will update **apt**, **python3**, and **pip**. Furthermore, **pip** will install all packages described in the **requirements.txt**, and execute **module.py**, which cleans the file **raw_data.csv**. The final step is to open this notebook file and make available every data from **clean_data.csv** as a **DataFrame**. \n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/dockerfile.png\" /></div>\n",
    "\n",
    "With the **Dockerfile** in place, run the build command, ***docker build -t username/project .***, which will create the image described on the **Dockerfile** in the local environment. One can push this new container to origin with ***docker push username/project*** to share with other developers or ***pull*** to another machine. Once the build succeeds, one can run the build with \n",
    "***docker run -p 8888:8888 username/project***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b9f3ea-1ca5-4faf-8ea2-900eebe53fa0",
   "metadata": {},
   "source": [
    "The following kernell should only work if executed inside the container, since the file **clean_data.csv** will only be created once the image is build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c339caa-194a-4e2b-9e61-a9d96b99e79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are not running this notebook inside the container, or the build was not a success.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lucas</td>\n",
       "      <td>24</td>\n",
       "      <td>Professor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pedro</td>\n",
       "      <td>28</td>\n",
       "      <td>Empresario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miguel</td>\n",
       "      <td>21</td>\n",
       "      <td>Advogado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  age         job\n",
       "0   Lucas   24   Professor\n",
       "1   Pedro   28  Empresario\n",
       "2  Miguel   21    Advogado"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('../data/clean_data.csv')\n",
    "    print('The build was a success, and the file is available')\n",
    "except:\n",
    "    print('You are not running this notebook inside the container, or the build was not a success.')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b99414-a744-47ce-98f9-957736278a2f",
   "metadata": {},
   "source": [
    "## AWS IAM Console"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaac198-2620-4cd4-947c-683eb0f7b980",
   "metadata": {},
   "source": [
    "The first thing we should do once we open **AWS Console** with **root user** is to create a new **IAM Group**. This step is very intuitive on the **AWS Console**. \n",
    "\n",
    "Firstly, we can create a group with Admin policies, or any policy of your choice. Secondly, we add a user to this group. This will be the user we will use to navigate **AWS** throughout the begining of this course. **P.S. Remember to save the new user credentials into a .csv file!!**\n",
    "\n",
    "To further understand **IAM Groups**, **IAM Roles**, **IAM policies**, and **IAM Users** read [AWS Identity and Access Management documentation](https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aed654e-ff78-45c5-926a-97f475be57e6",
   "metadata": {},
   "source": [
    "### AWS IAM Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f536aa06-918f-4875-b477-56be754e9d54",
   "metadata": {},
   "source": [
    "When thinking about IAM, there are two broad categories to consider: identities and permissions. \n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/iamstructure.png\" /></div>\n",
    "\n",
    "Identities refer to the various mechanisms that AWS provides to identify who is requesting a particular AWS action, for authenticating that person or entity, and for organizing similar entities into groups. Identities include users, groups, credentials, and roles. \n",
    "\n",
    "Permissions refer to what a particular identity is allowed to do in the AWS account. Permissions are managed by writing identity-based policies, which are collections of statements.\n",
    "\n",
    "I highly advise you to read the original documentation [here](https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce636a4d-37bf-4cee-9499-8c236509c008",
   "metadata": {},
   "source": [
    "### Managing AWS IAM with the Command Line Interface (CLI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b69af76-d7f7-4c58-9e66-4e0a4f230821",
   "metadata": {},
   "source": [
    "You will see throughout this course that using **AWS CLI** is the best choice you can make to interact with any service from **AWS**. For instance, let us list every user on our **AWS IAM Structure**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac48761-6b90-413d-857d-c16987be0b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Users\": [\n",
      "        {\n",
      "            \"Path\": \"/\",\n",
      "            \"UserName\": \"AdminUser\",\n",
      "            \"UserId\": \"AIDAREGJCUVZW77Z56TDT\",\n",
      "            \"Arn\": \"arn:aws:iam::077731112307:user/AdminUser\",\n",
      "            \"CreateDate\": \"2022-04-12T02:17:31Z\"\n",
      "        },\n",
      "        {\n",
      "            \"Path\": \"/\",\n",
      "            \"UserName\": \"S3FullAccessUser\",\n",
      "            \"UserId\": \"AIDAREGJCUVZ2V4WH7JGH\",\n",
      "            \"Arn\": \"arn:aws:iam::077731112307:user/S3FullAccessUser\",\n",
      "            \"CreateDate\": \"2022-04-10T14:43:33Z\",\n",
      "            \"PasswordLastUsed\": \"2022-04-10T20:06:52Z\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! aws iam list-users --profile adminuser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f452ed6b-5060-4a15-a609-992b19d6109b",
   "metadata": {},
   "source": [
    "Using the list argument, we are capable of listing roles, groups, and policies. Furthermore, one can create/delete every possible Identity. Manage policies and Identities permissions. To see all possible arguments, run the command ```aws iam help``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680fc900-e263-4238-8990-88a392103491",
   "metadata": {},
   "source": [
    "## AWS Cloud9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81df74f-3331-4d2c-a501-a867a4dddaaf",
   "metadata": {},
   "source": [
    "A development environment is a place in **AWS Cloud9** where you store your project's files and where you run the tools to develop your applications. One can easily create a new **Cloud9** Environment attached to an **EC2** via the **AWS Console**. The **Cloud9 IDE** is a frontend to the newly created **EC2**. You can use it to quickly deploy applications since you are already inside the **AWS Environment**.\n",
    "\n",
    "As soon as you open the **IDE**, go to the **sourcecontrol** tab on the left, and clone the repository of your choice. It is a good start to clone, yours truly, ***[https://github.com/Corbanez97/data_engineering_aws.git](https://github.com/Corbanez97/data_engineering_aws.git)***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a991a2be-5d49-4526-8e1b-cc5e8428ff3c",
   "metadata": {},
   "source": [
    "### Jupyter Lab on Cloud9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd59c723-e711-4e3d-a419-75b2d62d9f21",
   "metadata": {
    "tags": []
   },
   "source": [
    "Furthermore, it is possible to set up **Jupyter Lab** in this **EC2** For this, you must run all desired pip commands (***pip install jupyterlab***,***pip install addons***, **pip install themes***) to install **Jupyter Lab** once the **Cloud9 IDE** is open. From that, on the terminal, execute the command ***jupyter lab --ip 0.0.0.0 --port 8890***.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/cloud9jupyter.png\" /></div>\n",
    "\n",
    "Then, you should go **EC2's** console to edit its security group. Once you find yourself in the entry rules section, you edit these rules. As you can see, the last security group is set up to the previously described port 8890. \n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/securitygroups_jupyter.png\" /></div>\n",
    "\n",
    "This will allow us to connect to the **EC2's local host** using its **public IPv4 DNS** followed by \"colon port number\", for instance, ***[ec2-100-24-117-215.compute-1.amazonaws.com:8890](ec2-100-24-117-215.compute-1.amazonaws.com:8890)***. Just copy this address on the browser, and it will lead you to the **Jupyter Lab** hosted on the **EC2**! **(づ￣ ³￣)づ**\n",
    "\n",
    "If everything worked out fine, you should by now be seeing this notebook via the **AWS EC2**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697e1de5-13b1-45dc-aaca-d311ccfd556a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## AWS EC2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c6421-297e-48fa-88f8-e66ffc5ff966",
   "metadata": {},
   "source": [
    "Amazon Elastic Compute Cloud (Amazon EC2) provides scalable computing capacity in the Amazon Web Services (AWS) Cloud. Using Amazon EC2 eliminates your need to invest in hardware up front, so you can develop and deploy applications faster. You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. Amazon EC2 enables you to scale up or down to handle changes in requirements or spikes in popularity, reducing your need to forecast traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511f57e7-b3ea-46e8-85ec-c06d5d65066b",
   "metadata": {},
   "source": [
    "### AWS EC2 Local connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98516343-ccc1-476b-8282-9cefd0692c7d",
   "metadata": {},
   "source": [
    "It is also possible to connect to the EC2 using our local machine using a ssh key. To create a connection, run the following command:\n",
    "\n",
    "```\n",
    "corbanez@corbanez-H110M-H:~$ ssh-keygen\n",
    "\n",
    "```\n",
    "Which will generate the following output:\n",
    "\n",
    "```\n",
    "corbanez@corbanez-H110M-H:~$ ssh-keygen\n",
    "Generating public/private rsa key pair.\n",
    "Enter file in which to save the key (/home/corbanez/.ssh/id_rsa):\n",
    "\n",
    "```\n",
    "Make sure you save the ssh key inside the *.ssh* folder, given that if you do not do so, applications that use the key, could not accept it because of the permissions surrounding different folders. Next, you should go to the *~/.ssh* folder to get to key you've just created.\n",
    "\n",
    "```\n",
    "corbanez@corbanez-H110M-H:~$ cd ~/.ssh\n",
    "corbanez@corbanez-H110M-H:~/.ssh$ cat key_name.pub\n",
    "```\n",
    "The last command should return a MONSTRUOS string of random characters. Save it to the clipboard an open the **Cloud9 IDE**.\n",
    "\n",
    "On the **IDE**, we must add the newly created ssh key to our **EC2's *authorized_keys***. To do so, we shall open this file with **vi**, and add the MOUNTRUOS string right at the end.\n",
    "\n",
    "```\n",
    "ITVCloud9User:~ $ vi ~/.ssh/authorized_keys  \n",
    "```\n",
    "\n",
    "Just to make sure every thing turned out ok, run the command ```ITVCloud9User:~ $ tail -1 ~/.ssh/authorized_keys```. It should return, again, our MONSTRUOUS string of character. If this happens, we are good to go back to our local terminal.\n",
    "\n",
    "Back at the terminal, we must execute the following command using our **EC2 Instance public IPv4 DSN**:\n",
    "\n",
    "```\n",
    "corbanez@corbanez-H110M-H:~$ ssh -i ~/.ssh/ivc9user ec2-user@<EC2 public IPv4 DNS>\n",
    "\n",
    "```\n",
    "\n",
    "And if everything is **Ok**, we should be able to see a new **EC2** connection! **(♥‿♥)**\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/ec2_local_connex.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84894091-3932-43c2-bc63-8acaae162296",
   "metadata": {},
   "source": [
    "## AWS S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22be887a-2f57-4e5a-9dea-e598a1b3810d",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dc3b58-8def-4565-b2d8-143dcb1dadda",
   "metadata": {},
   "source": [
    "**Amazon S3** or **Amazon Simple Storage Service** is a service offered by **Amazon Web Services (AWS)** that provides object storage through a web service interface. **Amazon S3** uses the same scalable storage infrastructure that Amazon.com uses to run its global e-commerce network. It can be employed to store any type of object, which allows for uses like storage for Internet applications, backup and recovery, disaster recovery, data archives, data lakes for analytics, and hybrid cloud storage.[[Wiki]](https://en.wikipedia.org/wiki/Amazon_S3) \n",
    "\n",
    "**S3 Buckets** is one of the pillars of our data lake. That is where the data will arrive, be transformed, and archived. Therefore, understanding every possible type of storing data is a great way to begin creating Data Lakes and Data Warehouses. Take a look at the documentation describing different types of [**S3 Storage Classes**](https://aws.amazon.com/pt/s3/storage-classes/?nc=sn&loc=3).\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/s3_storage_classes.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84496307-3b5a-4e88-bb40-b1b479221d89",
   "metadata": {},
   "source": [
    "### Creation and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f247a221-1926-464e-b55c-3b2e48c32ad2",
   "metadata": {},
   "source": [
    "To create a **Bucket**, one must log in to the **AWS root user** and go to **S3 Services**. The path from here is very straightforward. Simply follow every big flashy button, giving names and descriptions of what may come. Furthermore, after creating the bucket, go to the **S3 Console** and create two folders (flashy buttons): ***landing*** and ***raw***.\n",
    "\n",
    "Once these two folders are setup, we should create a new **IAM Role** with permissions to List Buckets and Full Objects Access. Follow the steps described on the **AWS IAM Console** to create a new role with these permissions. Once you find yourself in the **Policies Panel**, go to the {}JSON mode, and paste the following code\n",
    "\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:s3:::<BUCKET_NAME>\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:*Object\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:s3:::<BUCKET_NAME>/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Create this **Policy**, and add it to the desired **IAM Group**. All users under this group should be able to list buckets and manipulate objects (saving, deleting and moving files inside of the bucket).\n",
    "\n",
    "The next step is to configure these new credentials in our local machine using **AWS CLI**. This is simply done by running the command \n",
    "\n",
    "```aws configure --profile <name_of_the_profile>```\n",
    "\n",
    "and use the **AWS Access Key ID**/**AWS Secret Access Key** from the **credentials.csv** file you saved once you created the user. Now, with the new profile, you can use the **AWS CLI** command to list folders inside the bucket of which the user has permissions. Just use the command\n",
    "\n",
    "```aws s3 ls s3://<BUCKET_NAME> --profile <name_of_the_profile>```.\n",
    "\n",
    "Since we've created two folders, if everything is working properly, you should receive the following output.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/s3_setup_awscli.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471e9cff-1d37-422d-bfd5-f86f2c564c68",
   "metadata": {},
   "source": [
    "### Cross-Region Replication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fc7e0-6e3e-4ae1-a306-65f5ad117833",
   "metadata": {},
   "source": [
    "For extra security, **AWS S3** has the great functionality of replicating a bucket throughout multiple regions. To have this feature in our data lake, we will create a new bucket with the same name as our current bucket (plus a suffix *copy*. Then, using an **IAM User** with  **AmazonS3FullAccess** we will set a **Replication Rule** for the main bucket. This rule will specify the prefix of data that will be copied, and the name of the bucket (```s3://<DUMP-BUCKET-NAME>```) which will be dumping our data. While creating the rule, you will be able to enable **Bucket Versioning** on the dump bucket, given you have the correct **IAM Role**.\n",
    "\n",
    "If you have set up this feature correctly, every change made to the main bucket should be seen on the destination bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b37241-56ba-429b-a480-bb752bab8414",
   "metadata": {},
   "source": [
    "### S3 Browsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4487f-3a90-4c30-8b9a-3c84fe9c85be",
   "metadata": {},
   "source": [
    "There are many **[S3 Browsers](https://chrome.google.com/webstore/detail/my-s3-browser/lgkbddebikceepncgppakonioaopmbkk)** available for free on the internet. They are a simple way to interact with our buckets just like we interact with folders on our machine.\n",
    "\n",
    "To access the bucket you've just created, you will need to activate the **Static site hosting**. To do so, go to the **Bucket Console**, then properties, and down we go! The last panel will be deactivated. Go to edit, and select activate. Under the field **Index Document**, place ```index.html```. Finally, hit **Save changes**.\n",
    "\n",
    "On the **Static site hosting** panel, you will now se something like this:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/static_site_hosting.png\" /></div>\n",
    "\n",
    "The link with the format ```http://<BUCKET>.s3-website-<region>.amazonaws.com``` is the **Bucket's Endpoint URL**. With this **URL** and your user credentials, you are capable to browse your bucket just like a computer folder. Moreover, you can add, delete, and copy objects into and out of the bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3d40a9-adb1-49d8-a9c4-ee006b8f41fc",
   "metadata": {},
   "source": [
    "### S3 Bucket Versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e46fa7-6fd8-458d-8737-119178b9e5c8",
   "metadata": {},
   "source": [
    "Given a specific bucket, one can enable **AWS Versioning** capabilities. Versioning in **Amazon S3** is a means of keeping multiple variants of an object in the same bucket. You can use the **S3 Versioning** feature to preserve, retrieve, and restore every version of every object stored in your buckets. \n",
    "\n",
    "With versioning you can recover more easily from both unintended user actions and application failures. After versioning is enabled for a bucket, if **Amazon S3** receives multiple write requests for the same object simultaneously, it stores all of those objects. [[Doc]](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html)\n",
    "\n",
    "To apply this option in a bucket, just go to your bucket properties and see the first block on the console. Select **Edit** and enable **Versioning**. With **Bucker Versioning** enabled, we must create a **Life Cycle Rule**. Go to bucket management, and create a rule to start using this feature.\n",
    "\n",
    "These rules are a way to both secure and correctly clean our buckets or transition between **Storage Classes**. Given the number of days on which an object is deleted/transition after it is considered out of date, we make sure we are not spending too much money maintaining our storage. And, given the number of different versions we assure security in case we lost our updated data. [[Doc]](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html)\n",
    "\n",
    "**P.S.:** Bucket Versioning will also be enabled on every other bucker, given you have set up replication of your main storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb5367-4a1c-4275-8f5c-dbf5c5f62ebe",
   "metadata": {},
   "source": [
    "### Managing AWS S3 using AWS Command Line Interface (AWS CLI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e69c7-00a1-4310-bc4b-ee9994377163",
   "metadata": {},
   "source": [
    "One of the main ways to interact with **S3** is using **AWS CLI**. The command-line interface creates the possibility of scripting and algorithmically managing buckets and objects. See the [Documentation](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3/index.html) for deeper learning of different commands.\n",
    "\n",
    "As an example, we ca try listing buckets and objects with ```aws s3 ls s3://<BUCKET_NAME> --profile <profile_name>```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb52d6d-d449-480c-8bcb-d3c26fba6fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE archive/\n",
      "                           PRE ingestion/\n",
      "                           PRE landing/\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls s3://supernovae --profile s3user #Shell command running on Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24adf73-e682-42b6-af59-3d5da062d967",
   "metadata": {},
   "source": [
    "As an example, let us create a *analytics* folder which will be used in the future to store **.parquet** file for **AWS Athena**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0503d32-8193-4514-9712-269f0bd78196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE analytics/\n",
      "                           PRE archive/\n",
      "                           PRE ingestion/\n",
      "                           PRE landing/\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p analytics/test.parquet #aws s3 does not copy empty directories to bucket.\n",
    "\n",
    "#cp is a key for copy on AWS CLI; With the --recursive argument, we are passing every subdirectory\n",
    "! aws s3 cp analytics s3://supernovae/analytics --recursive --profile s3user \n",
    "! aws s3 ls s3://supernovae --profile s3user #list new folders\n",
    "\n",
    "! rm -r analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b293f02-38ba-4138-9b59-1a7420702014",
   "metadata": {},
   "source": [
    "### Interacting with S3 using AWS Python Software Development Kit (Boto3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d8e7eb-5b27-4198-9531-d5f3877cb581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
